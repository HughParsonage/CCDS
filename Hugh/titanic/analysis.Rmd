---
title: "Titanic"
author: "Hugh"
date: "22 November 2016"
output: html_document
---

# Load the data
We need to use relative paths to direct `fread` to `train.csv`. And it needs to be relative to the `Rmd` file. 

```{r}
# Create a new object in this chunk
# called titanic_training that comprises
# the train.csv file from data-raw/titanic/
library(data.table)
library(dplyr)
library(dtplyr)
library(magrittr)
titanic_training <- fread("../../data-raw/titanic/train.csv")
```

If you don't know what package a particular functions comes from, (but you know it comes from an installed package -- and you haven't misspelled it), you can type `??thefunction`. Sometimes this comes up with only a few options.

```{r}
titanic_training %>%
  select(Survived)
```

Let's start with a basic model. This model just takes the average rate of survival and predicts that.

```{r}
# lm = linear model
lm.cointoss <- lm(Survived ~ 1, data = titanic_training)
```

`lm` has a `summary` method:

```{r}
summary(lm.cointoss)
```

Need to test the model (against actual values). Firstly, let's just test against the data we used to construct the model. To generate predictions given a model, use `predict(model, ...)`

```{r}
titanic_training_with_cointoss_prediction <- 
  titanic_training %>%
  mutate(predicted = predict(lm.cointoss, newdata = .))
```

The `error' of this model is what? One way is to use the *root mean-squared error*. That is:

```{r}
titanic_training_with_cointoss_prediction %$%
  sqrt(sum((Survived - predicted)^2) / length(Survived))
```

```{r}
titanic_training_with_cointoss_prediction %$%
  mean(abs(Survived - predicted))
```

What about a model where everyone dies?

```{r}
titanic_training %>%
  mutate(predicted = 0) %$%
  sqrt(sum((Survived - predicted)^2) / length(Survived))
```

This model is worse: it has a higher error rate (using the same error function). 

This sort of error we calculated is called **training error**. 

There are a few problems with the above model scenario. One is that people either survive or die: you can't 38 per cent survive. Likewise, you can't evaluate a model with categorical outputs using a continuous function:

```{r}
rmse <- function(predicted, actual){
  sqrt(sum((predicted - actual)^2) / length(actual))
}
```

### Exercises:
1. Instead of choosing 38 per cent for every row, choose 1 or 0 using `sample` with a 38 per cent chance of 1 as your prediction.

```{r}
titanic_training %>%
  mutate(predicted = sample(c(0, 1), 
                            size = nrow(.), 
                            replace = TRUE, 
                            prob = c(1 - mean(.$Survived),
                                     mean(.$Survived))))
```

2. Create a different function from `rmse` that takes two inputs and returns a single value: the misclasification error.

```{r}
mce <- function(actual, predicted){
  mean(actual != predicted)
}
```

3. Calculate the error using this function for your model in 1.
```{r}
titanic_training %>%
  mutate(predicted = sample(c(0, 1), 
                            size = nrow(.), 
                            replace = TRUE, 
                            prob = c(1 - mean(.$Survived),
                                     mean(.$Survived)))) %$%
  mce(Survived, predicted)
```

Using the `mce` on a model that always predicted death.

```{r}
titanic_training %>%
  mutate(predicted = 0) %$%
  mce(Survived, predicted)
```

So the latter model is better. 

There's a more serious problem with this approach to modelling. 

```{r}
library(rpart)
rpart.naive <- 
  rpart(Survived ~ ., data = titanic_training)

titanic_training %>%
  mutate(predicted = predict(rpart.naive, newdata = .)) %>%
  select(predicted, Survived) %$%
  mce(Survived, predicted)
```

This model above suffers from **overfitting**: it uses the names of people who died in the dataset to determine who will die next. Because the dataset we're using to test is identical to what it was trained on, it appears to perform well. In reality, it's just repeating what was in the data already -- and it will perform terribly on new data (with names not in the training data).

To get around this problem, we calculate the **test error** on *new data* -- data the model hasn't 'seen'. Typically we do this by partitioning up the data we have into a **training set** and a **test set**. Here is a common method:

```{r}
titanic_training_train <- 
  titanic_training %>%
  mutate(Name = factor(Name), 
         Ticket = factor(Ticket), 
         Cabin = factor(Cabin)) %>%
  # A random sample of 60% of the rows
  sample_frac(size = 0.6)
# An anti-join returns rows that DON'T match.
titanic_training_test <- 
  titanic_training %>%
  mutate(Name = factor(Name), 
         Ticket = factor(Ticket), 
         Cabin = factor(Cabin)) %>%
  # The 40% of the original dataset not in _train.
  anti_join(titanic_training_train)
```

Now:

```{r}
titanic_training_train.lm <- rpart(Survived ~ ., data = titanic_training_train)

titanic_training_test %>%
  mutate(predicted = predict(titanic_training_train.lm, newdata = .)) %$%
  mce(Survived, predicted)
```

# New model we could use more variables

```{r}
lm.2 <- lm(Survived ~ Sex + Fare, data = titanic_training)
```

# Exercise 
1. Deploy this model on `titanic_training`.
2. What is the **training error**?
3. How does this compare to an estimate of the **test error**?

```{r}
titanic_training %>%
  mutate(predicted = predict(lm.2, newdata = .))
```

An estimate of the training error:
```{r}
titanic_training %>%
  mutate(predicted = predict(lm.2, newdata = .)) %$%
  rmse(Survived, predicted)
```

3. Test error.

We can't use `lm.2`. Because we've used all of the data to train `lm.2`. There's a risk of overfitting. So we need to train the model, using the same specification, on a subset:
```{r}
lm.2a <- lm(Survived ~ Sex + Fare, data = titanic_training_train)
```

```{r}
titanic_training_test %>%
  mutate(predicted = predict(lm.2a, newdata = .)) %$%
  rmse(Survived, predicted)
```

Time-series:

```{r}
library(forecast)
```






